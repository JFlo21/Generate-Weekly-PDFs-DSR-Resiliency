"""
Advanced AI-Powered Audit Analysis Engine with Machine Learning
This module implements sophisticated machine learning models for intelligent audit analysis,
pattern recognition, anomaly detection, and predictive insights.
"""

# pylint: disable=all

import json
import datetime
import logging
import pickle
import os
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from dateutil import parser
import warnings
warnings.filterwarnings('ignore')

# Type annotations for better code clarity
from typing import Dict, List, Any, Optional, Union, Tuple
import multiprocessing
import os

# Optimize for GitHub Actions 4-core environment
if 'GITHUB_ACTIONS' in os.environ:
    # Use all available cores on GitHub Actions
    N_JOBS = min(4, multiprocessing.cpu_count())
else:
    # Conservative local setting
    N_JOBS = min(2, multiprocessing.cpu_count())

# Machine Learning Imports with fallback handling
ML_AVAILABLE = False
try:
    from sklearn.ensemble import RandomForestClassifier, IsolationForest  # type: ignore
    from sklearn.cluster import KMeans  # type: ignore
    from sklearn.preprocessing import StandardScaler, LabelEncoder  # type: ignore
    from sklearn.model_selection import train_test_split  # type: ignore
    from sklearn.metrics import classification_report, silhouette_score  # type: ignore
    from sklearn.decomposition import PCA  # type: ignore
    from sklearn.linear_model import LinearRegression  # type: ignore
    import joblib  # type: ignore
    ML_AVAILABLE = True
    logging.info("ðŸ¤– Machine Learning models loaded successfully")
except ImportError as e:
    logging.warning(f"Machine Learning libraries not available: {e}")
    # Define placeholder classes to prevent import errors
    class _DummyMLClass:
        def __init__(self, *args, **kwargs): pass
        def fit(self, *args, **kwargs): return self
        def predict(self, *args, **kwargs): return []
        def fit_predict(self, *args, **kwargs): return []
        def predict_proba(self, *args, **kwargs): return [[0.5, 0.5]]
        def score_samples(self, *args, **kwargs): return []
        def fit_transform(self, *args, **kwargs): return []
        def transform(self, *args, **kwargs): return []
        @property
        def cluster_centers_(self): return []
        @property
        def feature_importances_(self): return []
    
    # Assign dummy classes
    RandomForestClassifier = _DummyMLClass
    IsolationForest = _DummyMLClass  
    KMeans = _DummyMLClass
    StandardScaler = _DummyMLClass
    LabelEncoder = _DummyMLClass
    LinearRegression = _DummyMLClass
    
    class _DummyJoblib:
        def load(self, *args, **kwargs): return _DummyMLClass()
        def dump(self, *args, **kwargs): pass
    
    joblib = _DummyJoblib()
    
    def silhouette_score(*args, **kwargs): return 0.0

# NLP and Advanced AI Imports with fallback handling
NLP_AVAILABLE = False
try:
    from transformers import pipeline, AutoTokenizer, AutoModel  # type: ignore
    import torch  # type: ignore
    NLP_AVAILABLE = True
    logging.info("ðŸ§  NLP and Transformer models loaded successfully")
except ImportError as e:
    logging.warning(f"NLP libraries not available: {e}")
    def pipeline(*args, **kwargs): return None

# Visualization Imports
VISUALIZATION_AVAILABLE = False
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    import plotly.graph_objects as go
    import plotly.express as px
    VISUALIZATION_AVAILABLE = True
except ImportError as e:
    VISUALIZATION_AVAILABLE = False
    logging.warning(f"Visualization libraries not available: {e}")

class AdvancedAuditAIEngine:
    """
    Advanced AI-powered audit analysis engine with machine learning capabilities.
    Features:
    - Anomaly detection using Isolation Forest
    - Risk classification using Random Forest
    - User behavior clustering using K-Means
    - Time series pattern analysis
    - NLP-powered description generation
    - Predictive modeling for future violations
    """
    
    def __init__(self, model_dir="ai_models"):
        self.model_dir = model_dir
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.learning_data = []
        self.model_versions = {}
        
        # Ensure model directory exists
        os.makedirs(self.model_dir, exist_ok=True)
        
        # Initialize models
        if ML_AVAILABLE:
            self._initialize_ml_models()
        
        if NLP_AVAILABLE:
            self._initialize_nlp_models()
        
        # Load existing data and models
        self._load_learning_data()
        self._load_saved_models()
    
    def _initialize_ml_models(self):
        """Initialize machine learning models with GitHub Actions optimizations."""
        self.models = {
            'anomaly_detector': IsolationForest(contamination=0.1, random_state=42, n_jobs=N_JOBS),
            'risk_classifier': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=N_JOBS),
            'user_clusterer': KMeans(n_clusters=3, random_state=42),
            'pattern_analyzer': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=N_JOBS),
            'trend_predictor': LinearRegression(n_jobs=N_JOBS)
        }
        
        self.scalers = {
            'standard': StandardScaler(),
            'risk': StandardScaler(),
            'cluster': StandardScaler()
        }
        
        self.encoders = {
            'user': LabelEncoder(),
            'column': LabelEncoder(),
            'time': LabelEncoder()
        }
        
        logging.info("ðŸ¤– Machine Learning models initialized")
    
    def _initialize_nlp_models(self):
        """Initialize lightweight NLP models optimized for CPU execution."""
        try:
            # Use lightweight, CPU-optimized models for GitHub Actions
            # Small RoBERTa model for sentiment (much faster on CPU)
            self.nlp_sentiment = pipeline("sentiment-analysis", 
                                        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                                        device=-1)  # Force CPU usage
            
            # Use DistilBART for summarization (lighter than full BART)
            self.nlp_summarizer = pipeline("summarization", 
                                         model="sshleifer/distilbart-cnn-12-6",
                                         device=-1)  # Force CPU usage
            
            logging.info("ðŸš€ Lightweight CPU-optimized NLP models initialized")
        except Exception as e:
            logging.warning(f"NLP model initialization failed: {e}")
            # Fallback to rule-based approaches
            self.nlp_sentiment = None
            self.nlp_summarizer = None
            logging.info("ðŸ“ Using rule-based text analysis instead")
    
    def _load_learning_data(self):
        """Load historical learning data."""
        data_file = os.path.join(self.model_dir, "learning_data.json")
        try:
            if os.path.exists(data_file):
                with open(data_file, 'r') as f:
                    self.learning_data = json.load(f)
                logging.info(f"ðŸ“š Loaded {len(self.learning_data)} learning records")
        except Exception as e:
            logging.warning(f"Could not load learning data: {e}")
            self.learning_data = []
    
    def _save_learning_data(self):
        """Save learning data for future training."""
        data_file = os.path.join(self.model_dir, "learning_data.json")
        try:
            with open(data_file, 'w') as f:
                json.dump(self.learning_data, f, indent=2)
            logging.info(f"ðŸ’¾ Saved {len(self.learning_data)} learning records")
        except Exception as e:
            logging.error(f"Failed to save learning data: {e}")
    
    def _load_saved_models(self):
        """Load previously trained models."""
        for model_name in ['anomaly_detector', 'risk_classifier', 'user_clusterer']:
            model_file = os.path.join(self.model_dir, f"{model_name}.joblib")
            if os.path.exists(model_file):
                try:
                    self.models[model_name] = joblib.load(model_file)
                    logging.info(f"ðŸ“¦ Loaded pre-trained {model_name}")
                except Exception as e:
                    logging.warning(f"Could not load {model_name}: {e}")
    
    def _save_models(self):
        """Save trained models."""
        for model_name, model in self.models.items():
            if hasattr(model, 'fit'):  # Only save fitted models
                model_file = os.path.join(self.model_dir, f"{model_name}.joblib")
                try:
                    joblib.dump(model, model_file)
                    logging.info(f"ðŸ’¾ Saved {model_name}")
                except Exception as e:
                    logging.error(f"Failed to save {model_name}: {e}")
    
    def analyze_audit_data(self, audit_data, run_id):
        """
        Advanced AI analysis of audit data using machine learning models.
        """
        analysis_start = datetime.datetime.now()
        
        # Convert audit data to DataFrame for ML processing
        df = self._prepare_dataframe(audit_data)
        
        # Update learning data
        self._update_learning_data(audit_data, run_id)
        
        # Perform ML-powered analysis
        analysis_results = {
            'enhanced_data': [],
            'ml_insights': {},
            'anomalies': [],
            'user_clusters': {},
            'risk_predictions': {},
            'pattern_analysis': {},
            'trend_forecasts': {},
            'nlp_insights': {}
        }
        
        if ML_AVAILABLE and len(df) > 0:
            # 1. Anomaly Detection
            analysis_results['anomalies'] = self._detect_anomalies(df)
            
            # 2. Risk Classification
            analysis_results['risk_predictions'] = self._classify_risks(df)
            
            # 3. User Behavior Clustering
            analysis_results['user_clusters'] = self._cluster_users(df)
            
            # 4. Pattern Analysis
            analysis_results['pattern_analysis'] = self._analyze_patterns(df)
            
            # 5. Trend Forecasting
            analysis_results['trend_forecasts'] = self._forecast_trends(df)
        else:
            # Provide empty structures when ML is not available
            analysis_results['anomalies'] = {'anomaly_indices': [], 'anomaly_scores': []}
            analysis_results['risk_predictions'] = {'risk_scores': [], 'risk_classes': []}
            analysis_results['user_clusters'] = {'user_clusters': {}}
            analysis_results['pattern_analysis'] = {}
            analysis_results['trend_forecasts'] = {}
        
        if NLP_AVAILABLE:
            # 6. NLP-powered insights
            analysis_results['nlp_insights'] = self._generate_nlp_insights(audit_data)
        
        # Enhanced data with ML predictions
        for i, entry in enumerate(audit_data):
            enhanced_entry = entry.copy()
            
            # Add ML predictions if available
            if analysis_results['risk_predictions']:
                enhanced_entry['ml_risk_score'] = analysis_results['risk_predictions'].get('risk_scores', [0])[i] if i < len(analysis_results['risk_predictions'].get('risk_scores', [])) else 0
            
            if analysis_results['anomalies']:
                enhanced_entry['is_anomaly'] = i in analysis_results['anomalies'].get('anomaly_indices', [])
            
            # Generate intelligent description using ML insights
            enhanced_entry['ai_description'] = self._generate_ml_description(entry, analysis_results)
            
            analysis_results['enhanced_data'].append(enhanced_entry)
        
        # Generate comprehensive ML insights
        analysis_results['ml_insights'] = self._generate_ml_system_insights(df, analysis_results)
        
        # Save models and data
        if ML_AVAILABLE:
            self._save_models()
        self._save_learning_data()
        
        analysis_time = (datetime.datetime.now() - analysis_start).total_seconds()
        logging.info(f"ðŸ¤– Advanced ML analysis completed in {analysis_time:.2f}s")
        
        analysis_results['analysis_metadata'] = {
            'run_id': run_id,
            'analysis_time': analysis_time,
            'ml_models_used': len([m for m in self.models.values() if hasattr(m, 'predict')]),
            'anomalies_detected': len(analysis_results['anomalies'].get('anomaly_indices', [])) if isinstance(analysis_results['anomalies'], dict) else 0,
            'confidence_score': min(len(audit_data) * 15, 100)  # Higher confidence with more data
        }
        
        return analysis_results
    
    def _prepare_dataframe(self, audit_data):
        """Convert audit data to pandas DataFrame for ML processing."""
        df_data = []
        for entry in audit_data:
            row = {
                'delta': float(entry.get('delta', 0)),
                'abs_delta': abs(float(entry.get('delta', 0))),
                'changed_by': entry.get('changed_by', 'Unknown'),
                'column': entry.get('column', 'Unknown'),
                'work_request_number': entry.get('work_request_number', ''),
                'week_ending': entry.get('week_ending', ''),
                'changed_at': entry.get('changed_at', '')
            }
            
            # Extract time features
            try:
                change_time = parser.parse(entry.get('changed_at', ''))
                row['hour'] = change_time.hour
                row['day_of_week'] = change_time.weekday()
                row['is_weekend'] = change_time.weekday() >= 5
                row['is_after_hours'] = change_time.hour < 6 or change_time.hour > 18
            except:
                row['hour'] = 12  # Default to business hours
                row['day_of_week'] = 1
                row['is_weekend'] = False
                row['is_after_hours'] = False
            
            df_data.append(row)
        
        return pd.DataFrame(df_data)
    
    def _detect_anomalies(self, df):
        """Detect anomalies using Isolation Forest."""
        if len(df) < 5:  # Need minimum data for anomaly detection
            return {'anomaly_indices': [], 'anomaly_scores': []}
        
        # Prepare features for anomaly detection
        numeric_features = ['abs_delta', 'hour', 'day_of_week']
        X = df[numeric_features].fillna(0)
        
        # Scale features
        X_scaled = self.scalers['standard'].fit_transform(X)
        
        # Detect anomalies
        anomaly_pred = self.models['anomaly_detector'].fit_predict(X_scaled)
        anomaly_scores = self.models['anomaly_detector'].score_samples(X_scaled)
        
        # Get anomaly indices (where prediction = -1)
        anomaly_indices = [i for i, pred in enumerate(anomaly_pred) if pred == -1]
        
        return {
            'anomaly_indices': anomaly_indices,
            'anomaly_scores': anomaly_scores.tolist(),
            'anomaly_threshold': np.percentile(anomaly_scores, 10),  # Bottom 10% are anomalies
            'total_anomalies': len(anomaly_indices)
        }
    
    def _classify_risks(self, df):
        """Classify risk levels using Random Forest."""
        if len(df) < 10:  # Need minimum data for classification
            return {'risk_scores': [50] * len(df), 'risk_classes': ['MEDIUM'] * len(df)}
        
        # Create risk labels based on delta values
        risk_labels = []
        for delta in df['abs_delta']:
            if delta > 1000:
                risk_labels.append('HIGH')
            elif delta > 100:
                risk_labels.append('MEDIUM')
            else:
                risk_labels.append('LOW')
        
        # Prepare features
        feature_cols = ['abs_delta', 'hour', 'day_of_week', 'is_weekend', 'is_after_hours']
        X = df[feature_cols].fillna(0)
        
        # Encode categorical variables
        X_encoded = X.copy()
        
        # Scale features
        X_scaled = self.scalers['risk'].fit_transform(X_encoded)
        
        # Train if we have enough data diversity
        if len(set(risk_labels)) > 1:
            self.models['risk_classifier'].fit(X_scaled, risk_labels)
            
            # Predict probabilities
            risk_probs = self.models['risk_classifier'].predict_proba(X_scaled)
            risk_scores = np.max(risk_probs, axis=1) * 100  # Convert to percentage
            risk_classes = self.models['risk_classifier'].predict(X_scaled)
            
            return {
                'risk_scores': risk_scores.tolist(),
                'risk_classes': risk_classes.tolist(),
                'feature_importance': dict(zip(feature_cols, self.models['risk_classifier'].feature_importances_))
            }
        
        return {'risk_scores': [50] * len(df), 'risk_classes': risk_labels}
    
    def _cluster_users(self, df):
        """Cluster users based on behavior patterns."""
        if len(df) < 6:  # Need minimum data for clustering
            return {'user_clusters': {}, 'cluster_centers': []}
        
        # Aggregate user behavior
        user_stats = df.groupby('changed_by').agg({
            'abs_delta': ['mean', 'sum', 'count'],
            'is_after_hours': 'mean',
            'is_weekend': 'mean'
        }).fillna(0)
        
        # Flatten column names
        user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns]
        
        if len(user_stats) >= 3:  # Need at least 3 users for clustering
            # Scale features
            X_scaled = self.scalers['cluster'].fit_transform(user_stats)
            
            # Perform clustering
            cluster_labels = self.models['user_clusterer'].fit_predict(X_scaled)
            
            # Create user cluster mapping
            user_clusters = {}
            for user, cluster in zip(user_stats.index, cluster_labels):
                user_clusters[user] = {
                    'cluster': int(cluster),
                    'cluster_name': ['Low Risk', 'Medium Risk', 'High Risk'][cluster],
                    'behavior_score': float(np.mean(X_scaled[user_stats.index.get_loc(user)]))
                }
            
            return {
                'user_clusters': user_clusters,
                'cluster_centers': self.models['user_clusterer'].cluster_centers_.tolist(),
                'silhouette_score': silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else 0
            }
        
        return {'user_clusters': {}, 'cluster_centers': []}
    
    def _analyze_patterns(self, df):
        """Analyze patterns in the data using machine learning."""
        patterns = {
            'time_patterns': {},
            'user_patterns': {},
            'column_patterns': {},
            'correlation_analysis': {}
        }
        
        # Time pattern analysis
        hourly_stats = df.groupby('hour')['abs_delta'].agg(['count', 'mean', 'sum'])
        patterns['time_patterns'] = {
            'peak_hours': hourly_stats['count'].nlargest(3).to_dict(),
            'high_impact_hours': hourly_stats['mean'].nlargest(3).to_dict(),
            'after_hours_ratio': df['is_after_hours'].mean()
        }
        
        # User pattern analysis
        user_stats = df.groupby('changed_by')['abs_delta'].agg(['count', 'mean', 'sum'])
        patterns['user_patterns'] = {
            'most_active': user_stats['count'].nlargest(3).to_dict(),
            'highest_impact': user_stats['sum'].nlargest(3).to_dict(),
            'repeat_offenders': user_stats[user_stats['count'] > 1].index.tolist()
        }
        
        # Column pattern analysis
        column_stats = df.groupby('column')['abs_delta'].agg(['count', 'mean'])
        patterns['column_patterns'] = {
            'most_changed': column_stats['count'].to_dict(),
            'avg_impact': column_stats['mean'].to_dict()
        }
        
        # Correlation analysis
        numeric_cols = ['abs_delta', 'hour', 'day_of_week', 'is_after_hours', 'is_weekend']
        if len(df) > 10:
            correlations = df[numeric_cols].corr()['abs_delta'].drop('abs_delta').to_dict()
            patterns['correlation_analysis'] = correlations
        
        return patterns
    
    def _forecast_trends(self, df):
        """Forecast future trends using linear regression."""
        if len(df) < 5:
            return {'forecast': 'Insufficient data for forecasting'}
        
        # Create time-based features for forecasting
        df_sorted = df.sort_values('changed_at') if 'changed_at' in df.columns else df
        
        # Simple trend analysis based on violation frequency over time
        forecasts = {
            'violation_trend': 'stable',
            'expected_violations_next_period': len(df),
            'risk_trend': 'stable',
            'confidence': 'low'
        }
        
        if len(df) >= 10:
            # Calculate trend direction
            recent_half = df.tail(len(df)//2)
            earlier_half = df.head(len(df)//2)
            
            recent_avg_impact = recent_half['abs_delta'].mean()
            earlier_avg_impact = earlier_half['abs_delta'].mean()
            
            if recent_avg_impact > earlier_avg_impact * 1.2:
                forecasts['violation_trend'] = 'increasing'
                forecasts['expected_violations_next_period'] = int(len(df) * 1.3)
            elif recent_avg_impact < earlier_avg_impact * 0.8:
                forecasts['violation_trend'] = 'decreasing'
                forecasts['expected_violations_next_period'] = int(len(df) * 0.7)
            
            forecasts['confidence'] = 'medium' if len(df) >= 20 else 'low'
        
        return forecasts
    
    def _generate_nlp_insights(self, audit_data):
        """Generate NLP-powered insights from audit data."""
        if not NLP_AVAILABLE or not self.nlp_sentiment:
            return {'summary': 'NLP analysis not available'}
        
        # Create text summaries of violations
        violation_texts = []
        for entry in audit_data:
            delta = float(entry.get('delta', 0))
            column = entry.get('column', '')
            user = entry.get('changed_by', 'Unknown').split('@')[0]
            
            text = f"{user} made a {'positive' if delta > 0 else 'negative'} change of {abs(delta):.2f} to {column}"
            violation_texts.append(text)
        
        # Analyze sentiment of violations (positive = increases, negative = decreases)
        try:
            sentiment_results = []
            for text in violation_texts[:10]:  # Limit to avoid API overload
                result = self.nlp_sentiment(text)[0]
                sentiment_results.append(result)
            
            return {
                'sentiment_analysis': sentiment_results,
                'overall_sentiment': 'concerning' if len([r for r in sentiment_results if r['label'] == 'NEGATIVE']) > len(sentiment_results)/2 else 'mixed',
                'summary': f"Analyzed {len(sentiment_results)} violations with mixed sentiment patterns"
            }
        except Exception as e:
            logging.warning(f"NLP analysis failed: {e}")
            return {'summary': 'NLP analysis encountered errors'}
    
    def _generate_ml_description(self, entry, analysis_results):
        """Generate ML-powered intelligent description."""
        delta = float(entry.get('delta', 0))
        column = entry.get('column', '')
        user = entry.get('changed_by', 'Unknown')
        wr_num = entry.get('work_request_number', '')
        
        # Base description
        impact_level = "HIGH IMPACT" if abs(delta) > 1000 else "MODERATE IMPACT" if abs(delta) > 100 else "LOW IMPACT"
        direction = "increased" if delta > 0 else "decreased"
        
        # Add ML insights
        ml_context = ""
        if 'ml_risk_score' in entry:
            risk_score = entry.get('ml_risk_score', 50)
            ml_context += f" ðŸ¤– ML RISK SCORE: {risk_score:.1f}% - "
            if risk_score > 80:
                ml_context += "Algorithmic analysis flags this as HIGH RISK requiring immediate attention."
            elif risk_score > 60:
                ml_context += "Machine learning models classify this as ELEVATED RISK."
            else:
                ml_context += "AI assessment indicates MANAGEABLE RISK level."
        
        # Add anomaly detection
        if entry.get('is_anomaly', False):
            ml_context += " âš ï¸ ANOMALY DETECTED: This violation shows unusual patterns compared to historical data."
        
        # Add user clustering insights
        if user in analysis_results.get('user_clusters', {}).get('user_clusters', {}):
            cluster_info = analysis_results['user_clusters']['user_clusters'][user]
            cluster_name = cluster_info['cluster_name']
            ml_context += f" ðŸ‘¥ USER PROFILE: {user} classified as {cluster_name} based on behavioral patterns."
        
        # Generate final description
        if column == 'Quantity':
            base_desc = f"ðŸ”§ {impact_level}: Labor hours on Work Request #{wr_num} were {direction} by {abs(delta):.2f} hours after timesheet lock."
        elif column == 'Redlined Total Price':
            base_desc = f"ðŸ’° {impact_level}: Billing rate on Work Request #{wr_num} was {direction} by ${abs(delta):,.2f} after weekly cutoff."
        else:
            base_desc = f"ðŸ“ {impact_level}: Critical data on Work Request #{wr_num} was modified after lock period."
        
        return base_desc + ml_context
    
    def _generate_ml_system_insights(self, df, analysis_results):
        """Generate comprehensive ML system insights."""
        insights = {
            'anomaly_insights': [],
            'risk_insights': [],
            'user_insights': [],
            'pattern_insights': [],
            'predictive_insights': [],
            'recommendations': []
        }
        
        # Anomaly insights
        anomalies = analysis_results.get('anomalies', {})
        if isinstance(anomalies, dict) and anomalies.get('total_anomalies', 0) > 0:
            insights['anomaly_insights'].append(f"ðŸ” ANOMALY DETECTION: {anomalies['total_anomalies']} unusual patterns detected by machine learning algorithms")
            insights['anomaly_insights'].append(f"ðŸ“Š ANOMALY THRESHOLD: {anomalies.get('anomaly_threshold', 0):.3f} - values below this are flagged as anomalous")
        
        # Risk insights
        risk_preds = analysis_results.get('risk_predictions', {})
        if isinstance(risk_preds, dict) and risk_preds.get('risk_scores'):
            try:
                avg_risk = np.mean(risk_preds['risk_scores'])
                high_risk_count = len([s for s in risk_preds['risk_scores'] if s > 80])
                insights['risk_insights'].append(f"âš¡ RISK ANALYSIS: Average ML risk score is {avg_risk:.1f}% with {high_risk_count} high-risk violations")
                
                if 'feature_importance' in risk_preds:
                    top_feature = max(risk_preds['feature_importance'], key=risk_preds['feature_importance'].get)
                    insights['risk_insights'].append(f"ðŸŽ¯ KEY RISK FACTOR: '{top_feature}' is the strongest predictor of violation risk")
            except Exception as e:
                logging.warning(f"Risk insights calculation failed: {e}")
        
        # User clustering insights
        user_clusters = analysis_results.get('user_clusters', {})
        if isinstance(user_clusters, dict) and user_clusters.get('user_clusters'):
            try:
                cluster_counts = Counter([info['cluster_name'] for info in user_clusters['user_clusters'].values()])
                insights['user_insights'].append(f"ðŸ‘¥ USER CLUSTERING: {dict(cluster_counts)} - ML has identified distinct user behavior patterns")
                
                high_risk_users = [user for user, info in user_clusters['user_clusters'].items() if info['cluster_name'] == 'High Risk']
                if high_risk_users:
                    insights['user_insights'].append(f"ðŸš¨ HIGH-RISK USERS: {len(high_risk_users)} users classified as high-risk by clustering algorithm")
            except Exception as e:
                logging.warning(f"User clustering insights calculation failed: {e}")
        
        # Pattern insights
        patterns = analysis_results.get('pattern_analysis', {})
        if isinstance(patterns, dict) and patterns.get('time_patterns', {}).get('after_hours_ratio', 0) > 0.3:
            insights['pattern_insights'].append(f"ðŸ• TIME PATTERN: {patterns['time_patterns']['after_hours_ratio']:.1%} of violations occur after hours - security concern")
        
        # Predictive insights
        forecasts = analysis_results.get('trend_forecasts', {})
        if isinstance(forecasts, dict):
            if forecasts.get('violation_trend') == 'increasing':
                insights['predictive_insights'].append(f"ðŸ“ˆ TREND FORECAST: ML models predict {forecasts.get('expected_violations_next_period', 0)} violations next period (increasing trend)")
            elif forecasts.get('violation_trend') == 'decreasing':
                insights['predictive_insights'].append(f"ðŸ“‰ TREND FORECAST: ML models predict {forecasts.get('expected_violations_next_period', 0)} violations next period (decreasing trend)")
        
        # ML Recommendations
        insights['recommendations'] = [
            "ðŸ¤– ML ENHANCEMENT: Implement automated anomaly alerts when ML confidence > 85%",
            "ðŸ“Š DATA COLLECTION: Gather more historical data to improve ML model accuracy",
            "ðŸ”„ MODEL RETRAINING: Retrain models monthly with new violation data",
            "ðŸŽ¯ FEATURE ENGINEERING: Add more contextual features (project type, customer tier, etc.)",
            "âš¡ REAL-TIME SCORING: Implement live ML scoring for immediate violation assessment"
        ]
        
        return insights
    
    def _update_learning_data(self, audit_data, run_id):
        """Update the learning database with new audit data."""
        learning_entry = {
            'run_id': run_id,
            'timestamp': datetime.datetime.now().isoformat(),
            'violation_count': len(audit_data),
            'total_impact': sum(abs(float(e.get('delta', 0))) for e in audit_data),
            'users_involved': len(set(e.get('changed_by', '') for e in audit_data)),
            'high_risk_count': sum(1 for e in audit_data if abs(float(e.get('delta', 0))) > 1000)
        }
        
        self.learning_data.append(learning_entry)
        
        # Keep only last 100 entries to prevent unlimited growth
        self.learning_data = self.learning_data[-100:]
    
    def generate_ml_visualization_data(self, analysis_results):
        """Generate data formatted for creating ML visualizations."""
        viz_data = {
            'anomaly_chart': {},
            'risk_distribution': {},
            'user_clusters': {},
            'trend_analysis': {}
        }
        
        # Anomaly visualization data
        anomalies = analysis_results.get('anomalies', {})
        if anomalies.get('anomaly_scores'):
            viz_data['anomaly_chart'] = {
                'scores': anomalies['anomaly_scores'],
                'threshold': anomalies.get('anomaly_threshold', 0),
                'anomaly_indices': anomalies.get('anomaly_indices', [])
            }
        
        # Risk distribution
        risk_preds = analysis_results.get('risk_predictions', {})
        if risk_preds.get('risk_classes'):
            risk_counts = Counter(risk_preds['risk_classes'])
            viz_data['risk_distribution'] = dict(risk_counts)
        
        # User clusters
        user_clusters = analysis_results.get('user_clusters', {})
        if user_clusters.get('user_clusters'):
            cluster_data = {}
            for user, info in user_clusters['user_clusters'].items():
                cluster_name = info['cluster_name']
                if cluster_name not in cluster_data:
                    cluster_data[cluster_name] = []
                cluster_data[cluster_name].append({
                    'user': user.split('@')[0],  # Just the name part
                    'behavior_score': info['behavior_score']
                })
            viz_data['user_clusters'] = cluster_data
        
        # Trend analysis
        if self.learning_data:
            viz_data['trend_analysis'] = {
                'timestamps': [entry['timestamp'] for entry in self.learning_data[-10:]],
                'violation_counts': [entry['violation_count'] for entry in self.learning_data[-10:]],
                'impact_amounts': [entry['total_impact'] for entry in self.learning_data[-10:]]
            }
        
        return viz_data
